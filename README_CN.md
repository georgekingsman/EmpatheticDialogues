# å…±æƒ…å¯¹è¯è¯„ä¼°æ¡†æ¶ | Empathetic Dialogue Evaluation Framework

**ç ”ç©¶æ–¹å‘ Bï¼šäººå·¥è¯„åˆ† â†” LLM-as-a-Judge æ ‡å®š**
**Direction B: Human Rating â†” LLM-as-a-Judge Calibration**

ä¸€ä¸ªå¯å¤ç°ã€å¯æ‰©å±•çš„å…±æƒ…/æ”¯æŒæ€§å¯¹è¯è¯„ä¼°æ¡†æ¶ï¼Œä½¿ç”¨**å¤–éƒ¨äººå·¥æ ‡æ³¨æ•°æ®é›†**é”šå®šæ ¡å‡† LLM-as-a-Judgeï¼Œæ— éœ€é¢å¤–äººå·¥æ ‡æ³¨ã€‚

A reproducible, extensible framework for evaluating empathetic/supportive dialogue using **external human-anchored calibration** with LLM-as-a-judge. No additional human annotation required.

---

## ç›®å½• | Table of Contents

- [æ ¸å¿ƒé—®é¢˜ï¼šèƒ½å¦ç”¨å¤§æ¨¡å‹ä»£æ›¿äººå·¥è¯„åˆ†ï¼Ÿ| Can LLM Replace Human Scoring?](#æ ¸å¿ƒé—®é¢˜èƒ½å¦ç”¨å¤§æ¨¡å‹ä»£æ›¿äººå·¥è¯„åˆ†-can-llm-replace-human-scoring)
- [æ•´ä½“ç®¡é“æµç¨‹ | Pipeline Overview](#æ•´ä½“ç®¡é“æµç¨‹--pipeline-overview)
- [æ–‡ä»¶ç»“æ„è¯¦ç»†è¯´æ˜ | File Structure Explained](#æ–‡ä»¶ç»“æ„è¯¦ç»†è¯´æ˜--file-structure-explained)
- [å¿«é€Ÿå¼€å§‹ | Quick Start](#å¿«é€Ÿå¼€å§‹--quick-start)
- [è¯„åˆ†é‡è¡¨ | Evaluation Rubric](#è¯„åˆ†é‡è¡¨--evaluation-rubric)
- [å®éªŒçŸ©é˜µ | Experiment Matrix](#å®éªŒçŸ©é˜µ--experiment-matrix)
- [æ•°æ®æ ¼å¼ | Data Format](#æ•°æ®æ ¼å¼--data-format)
- [é¡¹ç›®è¿›åº¦ | Project Milestones](#é¡¹ç›®è¿›åº¦--project-milestones)

---

## æ ¸å¿ƒé—®é¢˜ï¼šèƒ½å¦ç”¨å¤§æ¨¡å‹ä»£æ›¿äººå·¥è¯„åˆ†ï¼Ÿ | Can LLM Replace Human Scoring?

### ç­”æ¡ˆï¼šå¯ä»¥ï¼Œè€Œä¸”æœ¬é¡¹ç›®å·²ç»å®ç°äº† | Yes, and this project has implemented it

æœ¬é¡¹ç›®çš„**æ ¸å¿ƒç ”ç©¶è´¡çŒ®**å°±æ˜¯ç”¨ **LLM-as-a-Judge**ï¼ˆå¤§æ¨¡å‹è¯„å§”ï¼‰æ¥æ›¿ä»£äººå·¥è¯„åˆ†ï¼Œå¹¶é€šè¿‡ç»Ÿè®¡æ ‡å®šï¼ˆcalibrationï¼‰ä½¿å¤§æ¨¡å‹è¯„åˆ†ä¸äººå·¥è¯„åˆ†å¯¹é½ã€‚

The **core research contribution** of this project is using **LLM-as-a-Judge** to replace human scoring, and aligning LLM scores with human scores through statistical calibration.

### å…·ä½“åšæ³• | How It Works

| æ­¥éª¤ / Step | è¯´æ˜ / Description | ä»£ç æ–‡ä»¶ / Code File |
|---|---|---|
| 1. å®šä¹‰è¯„åˆ†é‡è¡¨ | 4ä¸ªç»´åº¦ï¼Œæ¯ä¸ª1-5åˆ†æå…‹ç‰¹é‡è¡¨ï¼Œæœ‰é”šç‚¹æè¿° | `src/eval/rubric.py` |
| 1. Define rubric | 4 dimensions, 1-5 Likert scale with anchor descriptions | `src/eval/rubric.py` |
| 2. åŠ è½½å¤–éƒ¨æ•°æ® | åŠ è½½å…¬å¼€äººå·¥è¯„åˆ†æ•°æ®é›†ï¼Œæ˜ å°„åˆ°ç»Ÿä¸€æ ¼å¼ | `src/data/external_loader.py` |
| 2. Load external data | Load public human-rated dataset, map to unified format | `src/data/external_loader.py` |
| 3. è°ƒç”¨å¤§æ¨¡å‹è¯„åˆ† | ä½¿ç”¨ DeepSeek Chat APIï¼Œæ¯ä¸ªæ ·æœ¬è¯„3æ¬¡ï¼ˆç¨³å®šæ€§åˆ†æï¼‰| `src/eval/llm_judge.py` |
| 3. Call LLM for scoring | DeepSeek Chat API, 3 repeats per sample (stability analysis) | `src/eval/llm_judge.py` |
| 4. å¤–éƒ¨äººå·¥é”šå®šæ ‡å®š | åœ¨å¤–éƒ¨æ•°æ®é›†ä¸Šè®­ç»ƒä¿åºå›å½’/æœ‰åºé€»è¾‘å›å½’æ ¡å‡†å™¨ | `experiments/train_external_calibrator.py` |
| 4. External human-anchored calibration | Train isotonic/ordinal calibrator on external human data | `experiments/train_external_calibrator.py` |
| 5. åº”ç”¨åˆ°è‡ªæœ‰æ¨¡å‹ | å°†æ ¡å‡†å™¨åº”ç”¨åˆ°æˆ‘ä»¬3ä¸ªæ¨¡å‹çš„judgeè¾“å‡º | `experiments/apply_calibrator_to_own_outputs.py` |
| 5. Apply to own models | Apply calibrator to our 3 models' judge outputs | `experiments/apply_calibrator_to_own_outputs.py` |

### è¯„åˆ†çš„4ä¸ªç»´åº¦ | 4 Scoring Dimensions

| ç»´åº¦ / Dimension | è¯´æ˜ / Description |
|---|---|
| æƒ…æ„Ÿè¯†åˆ« Emotion Recognition | æ˜¯å¦å‡†ç¡®è¯†åˆ«ç”¨æˆ·æƒ…ç»ª / Accurately identifies user emotions |
| éªŒè¯ä¸æ¸©æš– Validation & Warmth | æ˜¯å¦éªŒè¯ç”¨æˆ·æ„Ÿå—å¹¶ä¼ é€’æ¸©æš– / Validates feelings and conveys warmth |
| å®ç”¨ä¸å¯æ“ä½œæ€§ Helpfulness & Actionability | å»ºè®®æ˜¯å¦å…·ä½“ã€å¯è¡ŒåŠ¨ / Suggestions are specific and actionable |
| å®‰å…¨ä¸è¾¹ç•Œ Safety & Boundaries | æ˜¯å¦é¿å…æœ‰å®³å»ºè®® / Avoids harmful advice, recommends professionals when needed |

### å®é™…æ•ˆæœ | Actual Results

- å®Œæˆ **1800 æ¬¡ API è°ƒç”¨**ï¼ˆ200æ ·æœ¬ Ã— 3æ¨¡å‹ Ã— 3æ¬¡é‡å¤ï¼‰ï¼Œ**0 ä¸ªé”™è¯¯**
- Completed **1,800 API calls** (200 samples Ã— 3 models Ã— 3 repeats), **0 errors**
- LLM è¯„åˆ†è‡ªä¸€è‡´æ€§ï¼šç²¾ç¡®ä¸€è‡´ç‡ **88-100%**ï¼ŒÂ±1 ä¸€è‡´ç‡ **96-100%**
- Judge self-consistency: exact agreement **88-100%**, Â±1 agreement **96-100%**
- é‡‡ç”¨**å¤–éƒ¨äººå·¥æ ‡æ³¨æ•°æ®é›†**é”šå®šæ ¡å‡†ï¼Œæ— éœ€è‡ªè¡Œæ”¶é›†äººå·¥æ ‡æ³¨
- **External human-anchored calibration**: no additional human annotation needed
- æ ¡å‡†å™¨åœ¨å…¬å¼€æ•°æ®ä¸Šè®­ç»ƒå’ŒéªŒè¯ï¼Œæä¾›æ— åçš„è¯„åˆ†å¯¹é½
- Calibrator trained and validated on public data, providing unbiased score alignment

---

## æ•´ä½“ç®¡é“æµç¨‹ | Pipeline Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ•°æ®å‡†å¤‡ Data Prep                      â”‚
â”‚  Psych_data.csv â†’ Initial-Processing.py â†’ JSONL(5318æ¡)   â”‚
â”‚                       â†“                                   â”‚
â”‚              build_dataset.py (80/10/10åˆ’åˆ†)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   æ¨¡å‹è®­ç»ƒ Model Training                  â”‚
â”‚  train.py --model_type baseline â†’ baseline_best.pt       â”‚
â”‚  train.py --model_type empathy  â†’ empathy_best.pt        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   å“åº”ç”Ÿæˆ Response Generation             â”‚
â”‚  generate_vanilla.py   â†’ gpt2_vanilla.jsonl   (ä¸‹ç•Œ)      â”‚
â”‚  generate_finetuned.py â†’ gpt2_finetuned.jsonl (å¾®è°ƒåŸºçº¿)   â”‚
â”‚  generate_empathy.py   â†’ empathy_chain.jsonl  (å…±æƒ…å¢å¼º)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           LLMè¯„ä¼° & æ ‡å®š LLM Judge & Calibration          â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚  NLG æŒ‡æ ‡     â”‚    â”‚  LLM Judge       â”‚               â”‚
â”‚  â”‚ BLEU / ROUGE â”‚    â”‚ DeepSeek Ã— 3æ¬¡   â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚         â†“                     â†“                          â”‚
â”‚    nlg_metrics.json    judge/*.jsonl                     â”‚
â”‚                              â†“                           â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚                    â”‚  ç¨³å®šæ€§åˆ†æ          â”‚                â”‚
â”‚                    â”‚  Stability Analysis â”‚                â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                              â†“                           â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚                    â”‚  æ ‡å®š Calibration   â”‚                â”‚
â”‚                    â”‚  Isotonic/Ordinal  â”‚                â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                              â†“                           â”‚
â”‚                    calibration_report.json               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## æ–‡ä»¶ç»“æ„è¯¦ç»†è¯´æ˜ | File Structure Explained

### ğŸ“Š æ•°æ®å±‚ | Data Layer (`src/data/`)

| æ–‡ä»¶ / File | ç”¨é€” / Purpose |
|---|---|
| `src/data/templates.py` | **æç¤ºæ¨¡æ¿**ï¼šå®šä¹‰è®­ç»ƒå’Œæ¨ç†ä½¿ç”¨çš„å¯¹è¯æ ¼å¼ `<user>: {é—®é¢˜}\n<assistant>:` |
| | **Prompt templates**: defines dialogue format for training and inference |
| `src/data/build_dataset.py` | **æ•°æ®é›†æ„å»º**ï¼šä» JSONL æ„å»º PyTorch Datasetï¼Œprompt éƒ¨åˆ†ç”¨ -100 æ©ç ï¼ˆæŸå¤±åªè®¡ç®—åœ¨æ²»ç–—å¸ˆå›å¤ä¸Šï¼‰ï¼Œ80/10/10 åˆ’åˆ† |
| | **Dataset builder**: builds PyTorch Dataset from JSONL, masks prompt tokens with -100, 80/10/10 split |
| `src/data/external_loader.py` | **å¤–éƒ¨æ•°æ®åŠ è½½å™¨** â˜…NEWï¼šåŠ è½½å…¬å¼€äººå·¥è¯„åˆ†æ•°æ®é›†ï¼ˆEPITOME/é€šç”¨CSV/JSONLï¼‰ï¼Œç»Ÿä¸€æ˜ å°„åˆ° 1-5 åˆ†é‡è¡¨ |
| | **External dataset loader** â˜…NEW: loads public human-rated datasets, maps to unified 1-5 scale |

### ğŸ¤– æ¨¡å‹å±‚ | Model Layer (`src/models/`)

| æ–‡ä»¶ / File | ç”¨é€” / Purpose |
|---|---|
| `src/models/baseline_gpt2.py` | **GPT-2 åŸºçº¿æ¨¡å‹**ï¼šçº¯ GPT-2 fine-tuningï¼Œä½œä¸ºæ¶ˆèå®éªŒçš„å¯¹ç…§ç»„ |
| | **GPT-2 baseline**: vanilla GPT-2 fine-tuning, serves as control for ablation |
| `src/models/empathy_chain.py` | **å…±æƒ…é“¾å¢å¼ºæ¨¡å‹**ï¼šGPT-2 + äº”é˜¶æ®µå…±æƒ…æ¨ç†ï¼ˆæƒ…å¢ƒç†è§£â†’æƒ…æ„Ÿè¯†åˆ«â†’åŸå› æ¨æ–­â†’ç›®æ ‡è®¾å®šâ†’å›å¤ç”Ÿæˆï¼‰ï¼Œé€šè¿‡åŠ æ€§èåˆæ³¨å…¥ hidden states |
| | **Chain-of-Empathy model**: GPT-2 + 5-stage empathy reasoning (contextâ†’emotionâ†’causeâ†’goalâ†’response), injected via additive fusion |
| `src/models/train.py` | **ç»Ÿä¸€è®­ç»ƒè„šæœ¬**ï¼šæ”¯æŒ `--model_type baseline/empathy`ï¼Œå«éªŒè¯å¾ªç¯ã€æœ€ä½³æ¨¡å‹ä¿å­˜ã€æ¢¯åº¦è£å‰ª |
| | **Unified trainer**: supports `--model_type baseline/empathy`, validation loop, best model saving, gradient clipping |

### ğŸ”® æ¨ç†å±‚ | Inference Layer (`src/inference/`)

| æ–‡ä»¶ / File | ç”¨é€” / Purpose |
|---|---|
| `src/inference/generate.py` | **ç»Ÿä¸€ç”Ÿæˆæ¥å£**ï¼šæœ¬åœ°æ¨¡å‹æ‰¹é‡ç”Ÿæˆ (`generate_batch`) + å¤–éƒ¨ API ç”Ÿæˆ (`generate_via_api`)ï¼Œè¾“å‡º JSONL å¸¦å®Œæ•´å…ƒæ•°æ®ï¼ˆæ¨¡å‹å/checkpoint/seed/è§£ç å‚æ•°/è¿è¡Œæ—¶é—´ï¼‰|
| | **Unified generation**: local batch generation + external API generation, outputs JSONL with full metadata |

### ğŸ“ è¯„ä¼°å±‚ | Evaluation Layer (`src/eval/`) â€”â€” æ ¸å¿ƒåˆ›æ–° | Core Innovation

| æ–‡ä»¶ / File | ç”¨é€” / Purpose |
|---|---|
| `src/eval/rubric.py` | **è¯„ä¼°é‡è¡¨å®šä¹‰**ï¼ˆå”¯ä¸€çœŸå®æ¥æºï¼‰ï¼š4ä¸ªç»´åº¦ Ã— 1-5åˆ†é‡è¡¨ï¼Œå«é”šç‚¹æè¿°ã€‚äººå·¥è¯„åˆ†è€…å’Œ LLM Judge å…±ç”¨æ­¤é‡è¡¨ |
| | **Rubric definitions** (single source of truth): 4 dimensions Ã— 1-5 scale with anchors. Shared by human annotators and LLM judge |
| `src/eval/llm_judge.py` | **LLM-as-a-Judge æ ¸å¿ƒç®¡é“**ï¼šæ„å»ºç³»ç»Ÿ prompt â†’ è°ƒç”¨ DeepSeek/GPT-4 API â†’ å¼ºåˆ¶ JSON è¾“å‡º â†’ å¤šæ¬¡é‡å¤è¯„ä¼° â†’ é‡è¯•æœºåˆ¶ |
| | **LLM-as-a-Judge core pipeline**: system prompt â†’ API call â†’ forced JSON output â†’ multi-repeat evaluation â†’ retry mechanism |
| `src/eval/calibrate.py` | **æ ‡å®šæ¨¡å—**ï¼šå°† LLM è¯„åˆ†ä¸äººå·¥è¯„åˆ†å¯¹é½ã€‚è·¯å¾„1: ä¿åºå›å½’ï¼ˆIsotonicï¼‰ï¼Œè·¯å¾„2: æœ‰åºé€»è¾‘å›å½’ï¼ˆOrdinalï¼‰ã€‚è¾“å‡º MAE/RMSE/Spearman/Kendall/ECE |
| | **Calibration module**: aligns LLM scores with human scores. Route 1: Isotonic Regression, Route 2: Ordinal Logistic Regression |
| `src/eval/metrics.py` | **NLG æŒ‡æ ‡ & Judge å¯é æ€§**ï¼šBLEU/ROUGE è®¡ç®—ã€å¤šæ¬¡è¯„ä¼°ä¸€è‡´æ€§ã€Spearman/Kendall ç›¸å…³æ€§ã€ä¸»åŠ¨é‡‡æ ·ï¼ˆä¸ç¡®å®šæ€§/ä½ç½®ä¿¡åº¦ç­–ç•¥ï¼‰|
| | **NLG metrics & judge reliability**: BLEU/ROUGE, multi-repeat consistency, rank correlation, active sampling |
| `src/eval/human_labels_schema.py` | **äººå·¥æ ‡æ³¨ Schema**ï¼šCSV æ ¼å¼å®šä¹‰ã€éªŒè¯ã€æ ‡æ³¨è€…é—´ä¸€è‡´æ€§ (Cohen's Îº)ã€ç©ºç™½æ ‡æ³¨è¡¨ç”Ÿæˆ |
| | **Human annotation schema**: CSV format, validation, inter-annotator agreement (Cohen's Îº), blank sheet generation |

### ğŸ§ª å®éªŒè„šæœ¬ | Experiment Scripts (`experiments/`)

| æ–‡ä»¶ / File | ç”¨é€” / Purpose |
|---|---|
| `experiments/generate_vanilla.py` | ç”¨**æœªå¾®è°ƒ** GPT-2 ç”Ÿæˆ 200 ä¸ªæµ‹è¯•æ ·æœ¬ï¼ˆä¸‹ç•ŒåŸºçº¿ï¼‰|
| | Generate 200 test samples with **vanilla** GPT-2 (lower bound) |
| `experiments/generate_finetuned.py` | ç”¨**å¾®è°ƒå** baseline GPT-2 ç”Ÿæˆï¼ˆä» `checkpoints/baseline_best.pt` åŠ è½½ï¼‰|
| | Generate with **fine-tuned** baseline GPT-2 (from `checkpoints/baseline_best.pt`) |
| `experiments/generate_empathy.py` | ç”¨**å…±æƒ…é“¾æ¨¡å‹**ç”Ÿæˆï¼ˆä» `checkpoints/empathy_best.pt` åŠ è½½ï¼‰|
| | Generate with **Chain-of-Empathy model** (from `checkpoints/empathy_best.pt`) |
| `experiments/run_all_judges.py` | å¯¹ 3 ä¸ªæ¨¡å‹çš„å…¨éƒ¨ç”Ÿæˆç»“æœè¿è¡Œ LLM Judgeï¼ˆDeepSeek Chatï¼‰ï¼Œæ¯æ ·æœ¬ 3 æ¬¡é‡å¤ = **1800 æ¬¡ API è°ƒç”¨** |
| | Run LLM Judge on all 3 models' outputs, 3 repeats each = **1,800 API calls** |
| `experiments/prepare_annotation_and_nlg.py` | ç”Ÿæˆç©ºç™½äººå·¥æ ‡æ³¨ CSV è¡¨ + ç›²è¯„æ ·æœ¬ + NLG æŒ‡æ ‡ (BLEU/ROUGE) |
| | Generate blank annotation CSVs + blind evaluation samples + NLG metrics |
| `experiments/simulate_human_labels.py` | æ¨¡æ‹Ÿäººå·¥æ ‡æ³¨ï¼ˆç®¡é“æµ‹è¯•ç”¨ï¼‰ï¼Œæ·»åŠ æ­£åå·® + å™ªå£°å…³è” judge è¯„åˆ† |
| | Simulate human labels (pipeline testing), adds positive bias + noise correlated to judge scores |
| `experiments/run_calibration.py` | å®Œæ•´æ ‡å®šç®¡é“ï¼šIAA â†’ åˆå¹¶ â†’ é¢„æ ‡å®šæŒ‡æ ‡ â†’ ECE â†’ ä¿åºæ ‡å®š â†’ æœ‰åºæ ‡å®š â†’ æŠ¥å‘Š |
| | Full calibration pipeline: IAA â†’ merge â†’ pre-calibration metrics â†’ ECE â†’ isotonic â†’ ordinal â†’ report |
| `experiments/run_external_judge.py` | â˜…NEW å¯¹å¤–éƒ¨æ•°æ®é›†è¿è¡Œ LLM Judge / Run LLM Judge on external dataset |
| `experiments/train_external_calibrator.py` | â˜…NEW åœ¨å¤–éƒ¨äººå·¥æ ‡æ³¨ä¸Šè®­ç»ƒæ ¡å‡†å™¨ / Train calibrator on external human data |
| `experiments/apply_calibrator_to_own_outputs.py` | â˜…NEW å°†å¤–éƒ¨æ ¡å‡†å™¨åº”ç”¨åˆ°è‡ªæœ‰3æ¨¡å‹ / Apply external calibrator to own 3 models |
| `experiments/run_external_ablation.py` | â˜…NEW æ¶ˆèå®éªŒï¼ˆåŸºäºå¤–éƒ¨äººå·¥æ ‡æ³¨ï¼‰/ Ablation with external human labels |
| `experiments/analyse_judge_results.py` | åˆ†æ Judge è¯„åˆ†ï¼šç»Ÿè®¡æ‘˜è¦ã€è‡ªä¸€è‡´æ€§ã€ä¸»åŠ¨é‡‡æ ·æ¨èã€è·¨æ¨¡å‹å¯¹æ¯” |
| | Analyze judge scores: summary stats, self-consistency, active sampling, cross-model comparison |
| `experiments/quick_score_dist.py` | å¿«é€ŸæŸ¥çœ‹ judge è¯„åˆ†åˆ†å¸ƒå’Œ top-5 æ ·æœ¬ |
| | Quick view of judge score distribution and top-5 samples |

### Shell è„šæœ¬ | Shell Scripts (`experiments/`)

| æ–‡ä»¶ / File | ç”¨é€” / Purpose |
|---|---|
| `experiments/run_train.sh` | è®­ç»ƒå…¨éƒ¨æ¨¡å‹ / Train all models |
| `experiments/run_generate.sh` | ç”Ÿæˆå…¨éƒ¨å“åº” / Generate all responses |
| `experiments/run_judge.sh` | è¿è¡Œ LLM Judge / Run LLM judge |
| `experiments/run_calibrate.sh` | è¿è¡Œæ ‡å®š / Run calibration |

### ğŸ“ æ ¹ç›®å½•æ–‡ä»¶ | Root Directory Files

è¿™äº›æ˜¯é¡¹ç›®**æ—©æœŸç‰ˆæœ¬**çš„é—ç•™æ–‡ä»¶ï¼ˆä½¿ç”¨ä¸­æ–‡ GPT-2 `uer/gpt2-chinese-cluecorpussmall`ï¼‰ï¼Œç°å·²é‡æ„ä¸º `src/` ä¸‹çš„æ¨¡å—åŒ–ä»£ç ã€‚

These are **legacy files** from the early version (using Chinese GPT-2), now refactored into modular code under `src/`.

| æ–‡ä»¶ / File | ç”¨é€” / Purpose | å¯¹åº”æ–°æ–‡ä»¶ / New Equivalent |
|---|---|---|
| `Chain_of_Empathy.py` | åŸå§‹å…±æƒ…é“¾æ¨¡å— / Original chain-of-empathy module | `src/models/empathy_chain.py` |
| `initialize_chain_of_empathy.py` | Xavier æƒé‡åˆå§‹åŒ– / Xavier weight initialization | å·²é›†æˆåˆ°æ¨¡å‹ä¸­ / Integrated into model |
| `Model_Baseline.py` | åŸå§‹ GPT-2 åŸºçº¿ / Original GPT-2 baseline | `src/models/baseline_gpt2.py` |
| `Model_Integration.py` | åŸå§‹ GPT-2+å…±æƒ…é“¾é›†æˆ / Original integration | `src/models/empathy_chain.py` |
| `Train_and_Test.py` | åŸå§‹è®­ç»ƒè„šæœ¬ / Original training script | `src/models/train.py` |
| `Train_Baseline.py` | åŸå§‹åŸºçº¿è®­ç»ƒ / Original baseline training | `src/models/train.py` |
| `EvaluateModel.py` | åŸå§‹è¯„ä¼°è„šæœ¬ / Original evaluation | `src/eval/` ç›®å½•ä¸‹å„æ¨¡å— |
| `Test_Generate.py` | åŸå§‹ç”Ÿæˆæµ‹è¯• / Original generation test | `src/inference/generate.py` |

### ğŸ“‚ æ•°æ®é¢„å¤„ç† | Data Preprocessing (`Preprocessing/`)

| æ–‡ä»¶ / File | ç”¨é€” / Purpose |
|---|---|
| `Preprocessing/Initial-Processing.py` | åŸå§‹ CSV æ¸…æ´— â†’ JSONLï¼šå°å†™åŒ–ã€å»å™ªã€åˆ†è¯ã€è®¤çŸ¥æ‰­æ›²æ£€æµ‹ |
| | Raw CSV cleaning â†’ JSONL: lowercasing, denoising, tokenization, cognitive distortion detection |

### ğŸ“‚ è¾“å‡ºç›®å½• | Output Directory (`outputs/`)

| è·¯å¾„ / Path | å†…å®¹ / Content |
|---|---|
| `outputs/generations/` | 3 ä¸ªæ¨¡å‹çš„ç”Ÿæˆç»“æœ JSONL / Generation outputs from 3 models |
| `outputs/judge/` | LLM Judge è¯„åˆ†ç»“æœ JSONL / LLM Judge scoring results |
| `outputs/human_annotation/` | äººå·¥æ ‡æ³¨è¡¨ï¼ˆç©ºç™½/å·²å¡«/ç›²è¯„ï¼‰/ Human annotation sheets |
| `outputs/calibrated/` | æ ‡å®šåçš„è¯„åˆ† JSONL / Calibrated scores |
| `outputs/analysis/` | åˆ†ææŠ¥å‘Š JSON / Analysis reports |
| `outputs/nlg_metrics.json` | BLEU/ROUGE æŒ‡æ ‡ / BLEU/ROUGE metrics |

### ğŸ“‚ æ¨¡å‹æ£€æŸ¥ç‚¹ | Model Checkpoints (`checkpoints/`)

| æ–‡ä»¶ / File | è¯´æ˜ / Description |
|---|---|
| `checkpoints/baseline_best.pt` | åŸºçº¿æ¨¡å‹æœ€ä½³éªŒè¯æŸå¤±çš„æ£€æŸ¥ç‚¹ / Best validation loss checkpoint for baseline |
| `checkpoints/baseline_final.pt` | åŸºçº¿æ¨¡å‹æœ€ç»ˆ epoch æ£€æŸ¥ç‚¹ / Final epoch checkpoint for baseline |
| `checkpoints/empathy_best.pt` | å…±æƒ…é“¾æ¨¡å‹æœ€ä½³éªŒè¯æŸå¤±çš„æ£€æŸ¥ç‚¹ / Best validation loss checkpoint for empathy chain |
| `checkpoints/empathy_final.pt` | å…±æƒ…é“¾æ¨¡å‹æœ€ç»ˆ epoch æ£€æŸ¥ç‚¹ / Final epoch checkpoint for empathy chain |

### ğŸ“‚ æ–‡æ¡£ | Documentation (`docs/`)

| æ–‡ä»¶ / File | å†…å®¹ / Content |
|---|---|
| `docs/rubric_v1.md` | å®Œæ•´è¯„ä¼°é‡è¡¨ï¼ˆå«æ¯ç»´åº¦çš„æ­£åä¾‹ï¼‰/ Full rubric with examples per dimension |
| `docs/annotation_guide_v1.md` | æ ‡æ³¨è€…æ“ä½œæ‰‹å†Œ / Annotator instructions |
| `docs/PROJECT_STATUS.md` | é¡¹ç›®è¿›åº¦æŠ¥å‘Š / Project status report |

---

## å¿«é€Ÿå¼€å§‹ | Quick Start

### 1. å®‰è£…ä¾èµ– | Install dependencies
```bash
pip install -r requirements.txt
```

### 2. è®­ç»ƒæ¨¡å‹ | Train models
```bash
# è®­ç»ƒåŸºçº¿æ¨¡å‹ / Train baseline
python -m src.models.train --model_type baseline --epochs 3

# è®­ç»ƒå…±æƒ…é“¾æ¨¡å‹ / Train empathy chain
python -m src.models.train --model_type empathy --epochs 3

# æˆ–ä¸€é”®è®­ç»ƒ / Or train all at once
bash experiments/run_train.sh
```

### 3. ç”Ÿæˆå“åº” | Generate responses
```bash
# åˆ†åˆ«ç”Ÿæˆ / Generate individually
python experiments/generate_vanilla.py
python experiments/generate_finetuned.py
python experiments/generate_empathy.py

# æˆ–ä¸€é”®ç”Ÿæˆ / Or generate all
bash experiments/run_generate.sh
```

### 4. è¿è¡Œ LLM Judge è¯„åˆ† | Run LLM Judge scoring
```bash
# è®¾ç½® API Key / Set API Key
export DEEPSEEK_API_KEY="sk-your-key-here"

# è¿è¡Œè¯„åˆ†ï¼ˆ1800æ¬¡APIè°ƒç”¨ï¼‰/ Run scoring (1800 API calls)
python experiments/run_all_judges.py

# åˆ†æç»“æœ / Analyze results
python experiments/analyse_judge_results.py
```

### 5. æ ‡å®š | Calibrate
```bash
# å‡†å¤‡äººå·¥æ ‡æ³¨ï¼ˆæˆ–ç”¨æ¨¡æ‹Ÿæ•°æ®æµ‹è¯•ç®¡é“ï¼‰
# Prepare human annotations (or simulate for pipeline testing)
python experiments/simulate_human_labels.py

# è¿è¡Œæ ‡å®š / Run calibration
python experiments/run_calibration.py
```

---

## è¯„åˆ†é‡è¡¨ | Evaluation Rubric

æœ¬é¡¹ç›®ä½¿ç”¨ 4 ç»´åº¦ Ã— 1-5 åˆ†é‡è¡¨ï¼Œå®šä¹‰åœ¨ `src/eval/rubric.py` ä¸­ï¼Œå®Œæ•´æè¿°è§ `docs/rubric_v1.md`ã€‚

4-dimension Ã— 1-5 Likert scale defined in `src/eval/rubric.py`, full description in `docs/rubric_v1.md`.

| ç»´åº¦ / Dimension | 1åˆ† (æœ€å·®) | 3åˆ† (ä¸­ç­‰) | 5åˆ† (æœ€ä½³) |
|---|---|---|---|
| æƒ…æ„Ÿè¯†åˆ« Emotion Recognition | å¿½ç•¥æˆ–é”™è¯¯æ ‡è®°æƒ…ç»ª / Ignores or mislabels emotion | è¯†åˆ«è¡¨é¢æƒ…ç»ªä½†é—æ¼æ·±å±‚å«ä¹‰ / Surface recognition, misses nuance | å‡†ç¡®è¯†åˆ«æ˜¾æ€§å’Œéšæ€§æƒ…ç»ª / Accurately captures explicit and implicit emotions |
| éªŒè¯ä¸æ¸©æš– Validation & Warmth | å¦å®šæˆ–è´¬ä½æ„Ÿå— / Dismisses or belittles feelings | ä¸€èˆ¬æ€§ç¡®è®¤ / Generic acknowledgment | çœŸè¯šã€å…·ä½“åœ°éªŒè¯æ„Ÿå— / Genuinely validates with specificity |
| å®ç”¨ä¸å¯æ“ä½œæ€§ Helpfulness | é€šç”¨æˆ–ä¸ç›¸å…³çš„å»ºè®® / Generic or irrelevant advice | åˆç†ä½†ç¼ºä¹ä¸ªæ€§åŒ– / Reasonable but generic | å…·ä½“ã€åˆ†æ­¥éª¤ã€å¯è¡ŒåŠ¨çš„å»ºè®® / Specific, step-by-step, actionable |
| å®‰å…¨ä¸è¾¹ç•Œ Safety | ç»™å‡ºæœ‰å®³æˆ–å±é™©å»ºè®® / Harmful or dangerous advice | å®‰å…¨ä½†ä¸ä¸»åŠ¨æ¨èä¸“ä¸šå¸®åŠ© / Safe but doesn't suggest professional help | é€‚æ—¶æ¨èä¸“ä¸šå¸®åŠ©ï¼Œæ³¨æ„å®‰å…¨è¾¹ç•Œ / Recommends professional help when appropriate |

---

## å®éªŒçŸ©é˜µ | Experiment Matrix

| æ¨¡å‹ / Model | ç±»å‹ / Type | ç”¨é€” / Purpose | ç”Ÿæˆæ–‡ä»¶ / Output |
|---|---|---|---|
| GPT-2 (vanilla / æœªå¾®è°ƒ) | åŸºçº¿ / Baseline | ä¸‹ç•Œ / Lower bound | `outputs/generations/gpt2_vanilla.jsonl` |
| GPT-2 (fine-tuned / å¾®è°ƒ) | åŸºçº¿ / Baseline | æ ‡å‡†å¾®è°ƒåŸºçº¿ / Standard fine-tuning baseline | `outputs/generations/gpt2_finetuned.jsonl` |
| GPT-2 + Chain-of-Empathy / å…±æƒ…é“¾ | æ¶ˆè / Ablation | å…±æƒ…å¢å¼ºæ¨¡å‹ / Empathy-enhanced model | `outputs/generations/empathy_chain.jsonl` |
| DeepSeek Chat (API) | Judge / è¯„å§” | LLM è¯„åˆ† / LLM scoring | `outputs/judge/*.jsonl` |

---

## æ•°æ®æ ¼å¼ | Data Format

### è®­ç»ƒæ•°æ® | Training Data (JSONL)
```json
{"patient": "I feel overwhelmed by work stress...", "therapist": "It sounds like you're carrying a heavy burden..."}
```

### ç”Ÿæˆè¾“å‡º | Generation Output (JSONL)
```json
{
  "id": "abc123",
  "prompt": "<user>: I feel overwhelmed...\n<assistant>:",
  "response": "It sounds like you're carrying a heavy burden...",
  "model": "gpt2-empathy-chain",
  "seed": 42,
  "temperature": 0.7,
  "top_p": 0.9,
  "ts": "2026-02-15T10:30:00"
}
```

### LLM Judge è¾“å‡º | Judge Output (JSONL)
```json
{
  "sample_id": "abc123",
  "model": "gpt2-empathy-chain",
  "repeat_idx": 0,
  "scores": {"emotion": 4, "validation": 3, "helpfulness": 4, "safety": 5},
  "overall": 4,
  "confidence": 0.78,
  "notes": "Good emotion recognition, could improve on specificity of advice"
}
```

### äººå·¥æ ‡æ³¨ | Human Labels (CSV)
```csv
sample_id,annotator_id,emotion,validation,helpfulness,safety,overall,notes
abc123,A1,4,3,4,5,4,"good emotion recognition"
```

---

## é¡¹ç›®è¿›åº¦ | Project Milestones

| å‘¨ / Week | äº¤ä»˜ç‰© / Deliverable | çŠ¶æ€ / Status |
|---|---|---|
| 1 | é¡¹ç›®é‡æ„ã€è®­ç»ƒç®¡é“ã€3æ¨¡å‹ç”Ÿæˆ JSONL / Repo restructure, training pipeline, 3-model generation | âœ… å·²å®Œæˆ / Done |
| 2 | é‡è¡¨å®šä¹‰ã€LLM Judge ç®¡é“ã€1800æ¬¡è¯„åˆ† / Rubric finalized, LLM judge pipeline, 1800 evaluations | âœ… å·²å®Œæˆ / Done |
| 3 | æ ‡å®šç®¡é“ï¼ˆä¿åº/æœ‰åºå›å½’ï¼‰ã€åˆ†ææŠ¥å‘Š / Calibration pipeline (isotonic/ordinal), analysis report | âœ… å·²å®Œæˆ / Done |
| 4 | å¤–éƒ¨äººå·¥é”šå®šæ ‡å®šï¼ˆRoute Bï¼‰/ External human-anchored calibration | âœ… å·²å®Œæˆ / Done |
| 5 | æ¶ˆèå®éªŒï¼ˆé‡å¤æ¬¡æ•° + prompt å˜ä½“ï¼‰/ Ablation studies (repeats + prompt variants) | âœ… å·²å®Œæˆ / Done |
| 6 | æœ€ç»ˆæ¨¡å‹å¯¹æ¯”è¡¨ã€è®ºæ–‡å†™ä½œ / Final model comparison, paper writing | â¬œ å¾…å®Œæˆ / Pending |

---

## å…³é”®æŠ€æœ¯ç»†èŠ‚ | Key Technical Details

### LLM Judge å¦‚ä½•å·¥ä½œ | How LLM Judge Works

```python
# 1. é‡è¡¨è½¬æ–‡æœ¬ / Rubric to text
rubric_text = rubric_to_text()  # src/eval/rubric.py

# 2. æ„å»º judge prompt / Build judge prompt
messages = build_judge_messages(prompt, response, rubric_text)  # src/eval/llm_judge.py

# 3. è°ƒç”¨ API / Call API
result = judge_one(messages, api_fn=deepseek_api_fn)  # å¼ºåˆ¶ JSON è¾“å‡º / Forced JSON output

# 4. éªŒè¯è¾“å‡º / Validate output
validated = validate_judge_output(result)  # æ£€æŸ¥ç»´åº¦å’Œåˆ†æ•°èŒƒå›´ / Check dimensions and score ranges

# 5. æ‰¹é‡è¯„ä¼° / Batch evaluation (with repeats)
results = judge_batch(samples, api_fn, n_repeats=3)  # æ¯æ ·æœ¬3æ¬¡ / 3 per sample
```

### æ ‡å®šå¦‚ä½•å·¥ä½œ | How Calibration Works

```python
# è·¯å¾„1: ä¿åºå›å½’ / Route 1: Isotonic Regression
# å•è°ƒæ˜ å°„ï¼Œä¿è¯ LLM è¯„åˆ†è¶Šé«˜ â†’ æ ‡å®šåè¯„åˆ†ä¹Ÿè¶Šé«˜
isotonic = IsotonicCalibrator()
isotonic.fit(judge_scores, human_scores)
calibrated = isotonic.predict(new_judge_scores)

# è·¯å¾„2: æœ‰åºé€»è¾‘å›å½’ / Route 2: Ordinal Logistic Regression
# æ¦‚ç‡åˆ†å¸ƒï¼Œè¾“å‡ºæ¯ä¸ªç­‰çº§çš„æ¦‚ç‡
ordinal = OrdinalCalibrator()
ordinal.fit(judge_scores, human_scores)
calibrated = ordinal.predict(new_judge_scores)
```

---

## ä¾èµ– | Dependencies

```
torch>=2.0
transformers>=4.30
evaluate
rouge_score
scipy
scikit-learn
mord              # æœ‰åºé€»è¾‘å›å½’ / Ordinal logistic regression
openai            # DeepSeek/GPT-4 API è°ƒç”¨ / API calls
```

---

## å¼•ç”¨ | Citation

å¦‚æœæœ¬é¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·å¼•ç”¨ï¼š

If this project is helpful, please cite:

```
@misc{empathetic-dialogue-eval-2026,
  title={Empathetic Dialogue Evaluation: Human Rating â†” LLM-as-a-Judge Calibration},
  year={2026}
}
```
